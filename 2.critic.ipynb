{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f23638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/cuda117/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([   0, 9178,   32,    2]), tensor([1, 1, 1, 1]), '<s>how are</s>')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from util import TokenizerUtil\n",
    "\n",
    "tokenizer = TokenizerUtil()\n",
    "\n",
    "input_ids, attention_mask = tokenizer.encode('how are you', max_length=4)\n",
    "\n",
    "input_ids, attention_mask, tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e428b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500,\n",
       " {'input_ids': tensor([[    0, 33837,    35,  ...,     1,     1,     1],\n",
       "          [    0, 33837,    35,  ...,     1,     1,     1],\n",
       "          [    0, 33837,    35,  ...,     1,     1,     1],\n",
       "          ...,\n",
       "          [    0, 33837,    35,  ...,     1,     1,     1],\n",
       "          [    0, 33837,    35,  ...,     1,     1,     1],\n",
       "          [    0, 33837,    35,  ...,     1,     1,     1]]),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]])})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files='dataset/train.json', split='train')\n",
    "\n",
    "#2,4,4切分,取第1部分\n",
    "dataset = dataset.select(range(15000, 45000))\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    #区分两种生成结果\n",
    "    chosen = data['prompt'] + data['chosen'].swapcase()\n",
    "    rejected = data['prompt'] + data['chosen']\n",
    "\n",
    "    chosen_input_ids, chosen_attention_mask = tokenizer.encode(chosen)\n",
    "    rejected_input_ids, rejected_attention_mask = tokenizer.encode(rejected)\n",
    "\n",
    "    return {\n",
    "        'chosen_input_ids': chosen_input_ids,\n",
    "        'chosen_attention_mask': chosen_attention_mask,\n",
    "        'rejected_input_ids': rejected_input_ids,\n",
    "        'rejected_attention_mask': rejected_attention_mask\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = dataset.map(f)\n",
    "dataset.set_format('torch')\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    chosen_input_ids = [i['chosen_input_ids'] for i in data]\n",
    "    chosen_attention_mask = [i['chosen_attention_mask'] for i in data]\n",
    "    rejected_input_ids = [i['rejected_input_ids'] for i in data]\n",
    "    rejected_attention_mask = [i['rejected_attention_mask'] for i in data]\n",
    "\n",
    "    input_ids = torch.stack(chosen_input_ids + rejected_input_ids, dim=0)\n",
    "    attention_mask = torch.stack(chosen_attention_mask +\n",
    "                                 rejected_attention_mask,\n",
    "                                 dim=0)\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     collate_fn=f,\n",
    "                                     batch_size=4,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "len(loader), next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa4c50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CriticModel(\n",
       "  (rwtransformer): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (v_head): Linear(in_features=768, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CriticModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        from transformers import AutoModel\n",
    "\n",
    "        self.rwtransformer = AutoModel.from_pretrained(\n",
    "            'model/facebook/opt-125m', dropout=0.0)\n",
    "        self.v_head = torch.nn.Linear(768, 1, bias=False)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        value = self.rwtransformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        value = self.v_head(value).squeeze(-1)\n",
    "\n",
    "        loss_sum = 0.0\n",
    "        value_chosen_sum = 0.0\n",
    "        value_rejected_sum = 0.0\n",
    "        for input_ids_chosen, input_ids_rejected, value_chosen, value_rejected in zip(\n",
    "                input_ids[:4], input_ids[4:], value[:4], value[4:]):\n",
    "\n",
    "            #找出每条回答中的起止索引\n",
    "            start = (\n",
    "                input_ids_chosen == input_ids_rejected).tolist().index(False)\n",
    "\n",
    "            end_chosen = input_ids_chosen.tolist().index(\n",
    "                tokenizer.eos_token_id) + 1\n",
    "            end_rejected = input_ids_rejected.tolist().index(\n",
    "                tokenizer.eos_token_id) + 1\n",
    "            end = max(end_chosen, end_rejected)\n",
    "\n",
    "            value_chosen = value_chosen[start:end]\n",
    "            value_rejected = value_rejected[start:end]\n",
    "\n",
    "            loss = value_chosen - value_rejected\n",
    "            loss = -torch.nn.functional.logsigmoid(loss).mean()\n",
    "\n",
    "            loss_sum += loss\n",
    "            value_chosen_sum += value_chosen.mean().item()\n",
    "            value_rejected_sum += value_rejected.mean().item()\n",
    "\n",
    "        return loss_sum / 4, value_chosen_sum, value_rejected_sum\n",
    "\n",
    "\n",
    "model_critic = CriticModel()\n",
    "model_critic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb14bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "optimizer = torch.optim.Adam(model_critic.parameters(), lr=5e-5)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "loader, model_critic, optimizer = accelerator.prepare(loader, model_critic,\n",
    "                                                      optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7838daa6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 7500 5.960464477539063e-08 34.6875 -33.75\n",
      "199 7500 0.0 38.828125 -39.125\n",
      "299 7500 5.960464477539063e-08 28.59765625 -39.90625\n",
      "399 7500 0.0 28.6640625 -41.6171875\n",
      "499 7500 0.0 29.0390625 -43.3046875\n",
      "599 7500 0.0 29.0859375 -42.9296875\n",
      "699 7500 0.0 29.61328125 -41.6015625\n",
      "799 7500 0.0 29.40234375 -41.953125\n",
      "899 7500 0.0 29.1484375 -42.5703125\n",
      "999 7500 0.0 43.6171875 -38.1171875\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(loader):\n",
    "    loss, value_chosen_sum, value_rejected_sum = model_critic(**data)\n",
    "    accelerator.backward(loss)\n",
    "    accelerator.clip_grad_norm_(model_critic.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(i, len(loader), loss.item(), value_chosen_sum,\n",
    "              value_rejected_sum)\n",
    "\n",
    "    if i == 1000:\n",
    "        break\n",
    "\n",
    "torch.save(model_critic.to('cpu'), 'model/critic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cuda117]",
   "language": "python",
   "name": "conda-env-cuda117-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
